{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled16.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KUVjuEk3rf-"
      },
      "source": [
        "Initialization cell : launch the following cell at the beginning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sMoTnDY3nYO"
      },
      "source": [
        "#initialization cell\n",
        "\n",
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "\n",
        "import sys\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "try:\n",
        "    from simplejson.errors import JSONDecodeError\n",
        "except ImportError:\n",
        "    from json.decoder import JSONDecodeError\n",
        "\n",
        "\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbLAPYEL32Tw"
      },
      "source": [
        "Please enter your parameters in the following cell : your Legifrance token and the path in which your scrapped database will be saved (note that the path should be under the form '/yourpath/folder' without a '/' at the end). Please type the subject of agreement that you want to scrap ('equa' for Professional equality or 'tele' for Telecommuting) and the type (scrapped by 'title' or by 'theme')."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meB-qSzM4Rgo"
      },
      "source": [
        "token_legifrance = 'YourLegifranceToken'\n",
        "save_path = 'enter/your/path'\n",
        "\n",
        "agree_subject = 'tele'\n",
        "# agree_subject = 'equa'\n",
        "\n",
        "agree_type = 'title'\n",
        "# agree_type = 'theme'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5DCJsxy5Cra"
      },
      "source": [
        "Following next : the cell containing all the methods to scrap the agreements. Nothing will happen, the scrapping will beginning the cell after this one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJseNIB34-hh"
      },
      "source": [
        "class ScrapAgreement:\n",
        "    \"\"\"A class for creating scrappers on Legifrance API.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, agree_subject='tele', agree_type='title', token_legifrance=None):\n",
        "        \"\"\"ScrapAgreement class constructor.\n",
        "        Parameters\n",
        "        ----------\n",
        "        agree_subject : str\n",
        "            Must be 'tele' for scrapping Telecommuting agreements or 'equa' for Professional Equality agreements.\n",
        "            (Default value = 'tele')\n",
        "        agree_type : str\n",
        "            Must be 'title' for scrapping agreements according to their title or 'theme' for scrapping them according to\n",
        "            their theme. (Default value = 'title')\n",
        "        token_legifrance : int_or_str\n",
        "            Token obtained on Legifrance API. More details on how to get it in the Readme file. (Default value = None)\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If the agree_subject or agree_type are not in str format such as described above. Or if the user has not\n",
        "            entered a valid `token_legifrance`.\n",
        "        \"\"\"\n",
        "        self.agree_subject = agree_subject\n",
        "        self.agree_type = agree_type\n",
        "\n",
        "        # Next, we choose the template of the url that will be used for scrapping, according to the subject and type of\n",
        "        # agreement wished by the user.\n",
        "        if agree_subject == 'tele':\n",
        "            if agree_type == 'title':\n",
        "                self.url_scrap = 'https://www.legifrance.gouv.fr/search/acco?tab_selection=acco&searchField=TITLE' \\\n",
        "                                 '&query=t%C3%A9l%C3%A9travail&searchType=ALL&typePagination=DEFAULT&sortValue' \\\n",
        "                                 '=DATE_ASC&pageSize=100&page={page_number}&tab_selection=acco#acco '\n",
        "            elif agree_type == 'theme':\n",
        "                self.url_scrap = 'https://www.legifrance.gouv.fr/search/acco?tab_selection=acco&searchField=ALL' \\\n",
        "                                 '&searchType=ALL&theme=yDGL3w%3D%3D&typePagination=DEFAULT&sortValue=DATE_ASC' \\\n",
        "                                 '&pageSize=100&page={page_number}&tab_selection=acco#acco '\n",
        "            else:\n",
        "                raise ValueError('agree_type must be set as \"tele\" or \"title\".')\n",
        "        elif agree_subject == 'equa':\n",
        "            if agree_type == 'title':\n",
        "                self.url_scrap = 'https://www.legifrance.gouv.fr/search/acco?tab_selection=acco&searchField=TITLE' \\\n",
        "                                 '&query=%C3%A9galit%C3%A9+professionnelle&searchType=ALL&typePagination=DEFAULT' \\\n",
        "                                 '&sortValue=DATE_ASC&pageSize=100&page={page_number}&tab_selection=acco#acco '\n",
        "            elif agree_type == 'theme':\n",
        "                self.url_scrap = 'https://www.legifrance.gouv.fr/search/acco?tab_selection=acco&searchField=ALL' \\\n",
        "                                 '&searchType=ALL&theme=iOR56g%3D%3D&typePagination=DEFAULT&sortValue=DATE_ASC' \\\n",
        "                                 '&pageSize=100&page={page_number}&tab_selection=acco#acco '\n",
        "            else:\n",
        "                raise ValueError('agree_type must be set as \"tele\" or \"title\".')\n",
        "        else:\n",
        "            raise ValueError('agree_subject must be set as \"tele\" or \"equa\".')\n",
        "\n",
        "        if token_legifrance is None:\n",
        "            raise ValueError('Please go on Legifrance API and fetch a token there. Go on the Readme file for further '\n",
        "                             'details on how to do it.')\n",
        "        self.token_legifrance = token_legifrance\n",
        "\n",
        "    def get_max_page_number(self):\n",
        "        \"\"\"Creates a max_page_number attribute, which corresponds to the number of pages for the search results on\n",
        "        Legifrance.\n",
        "        \"\"\"\n",
        "        first_page_url = self.url_scrap.format(page_number=1)\n",
        "        wd = webdriver.Chrome('chromedriver', chrome_options=chrome_options)\n",
        "        wd.get(first_page_url)\n",
        "        source = wd.page_source\n",
        "        soup = BeautifulSoup(source)\n",
        "        self.max_page_number = int(soup.select('#main_acco > div > div > div > nav > ul > li > a')[2][\n",
        "                                       'data-num'])  # Total number of pages on legifrance for the given search results.\n",
        "\n",
        "    @staticmethod\n",
        "    def get_a_page(soup):\n",
        "        \"\"\"Gets a list of dictionaries of agreements IDs and links on a given Legifrance page.\n",
        "        Parameters\n",
        "        ----------\n",
        "        soup (BeautifulSoup object):\n",
        "            The soup object of a given page of agreements from Legifrance.\n",
        "        Returns\n",
        "        -------\n",
        "        list(dict)\n",
        "            A list of dictionaries each containing an unique ID and link for each agreement on a chosen page.\n",
        "        \"\"\"\n",
        "        list_agreement_page = []\n",
        "        for text in soup.findAll(\"h2\"):\n",
        "            agreement_id = text['data-id']\n",
        "            link = 'https://www.legifrance.gouv.fr' + text.findChild(\"a\")['href']\n",
        "            agreement = {'link': link, 'id': agreement_id}\n",
        "            list_agreement_page.append(agreement)\n",
        "\n",
        "        return list_agreement_page\n",
        "\n",
        "    def get_all_pages_ids(self, n=1_000_000):\n",
        "        \"\"\"Generalizes `ScrapAgreement.get_a_page()` method to all pages on Legifrance according to the scrapping\n",
        "        criteria chosen by the user (subject and type).\n",
        "        Creates a list_agreement attribute (for your ScrapAgreement object) which consists in a list of dictionaries of\n",
        "        IDs and links for each agreement.\n",
        "        They are fetched in chronological order.\n",
        "        Parameters\n",
        "        ----------\n",
        "        n :\n",
        "            Maximum number of pages that will be scrapped. Is set to 1 Million by default, i.e. it fetches every page by\n",
        "            default. (Default value = 1_000_000)\n",
        "        \"\"\"\n",
        "        if n == 1_000_000:\n",
        "            # In case the user did not specify a number of page he is willing to scrap, we will automatically fetch\n",
        "            # the maximum number of pages.\n",
        "            self.get_max_page_number()\n",
        "            n = self.max_page_number\n",
        "        list_agreement = []\n",
        "        for i in range(1, n + 1):\n",
        "            url = self.url_scrap.format(page_number=i)\n",
        "            wd.get(url)\n",
        "            source = wd.page_source\n",
        "            soup = BeautifulSoup(source)\n",
        "            try:\n",
        "                list_agreement_page_i = self.get_a_page(soup)\n",
        "                list_agreement.extend(list_agreement_page_i)\n",
        "            except:\n",
        "                print(i)\n",
        "                print(f\"Error getting agreement {i} id.\")\n",
        "        self.list_agreement = list_agreement\n",
        "\n",
        "    @staticmethod\n",
        "    def get_company_size(siret):\n",
        "        \"\"\"Gets the size and the size category of a French company using its Siret, and by scrapping the Sirene website.\n",
        "        Used website : https://www.sirene.fr/sirene/public/accueil\n",
        "        Parameters\n",
        "        ----------\n",
        "        siret : int_or_str\n",
        "            Siret of the company you wish to get the size.\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            Range of people in the company (e.g. '150 à 999 personnes').\n",
        "        str\n",
        "            Size category of the company, according to INSEE taxonomy ('PME', 'ETI' or 'GE'). For more, see :\n",
        "            https//www.insee.fr/fr/metadonnees/definition/c1057.\n",
        "        \"\"\"\n",
        "        url_siret = f'https://www.sirene.fr/sirene/public/recherche?recherche.sirenSiret=54565020200025&amp;recherche' \\\n",
        "                    f'.commune=&amp;recherche.captcha=&amp;__checkbox_recherche.excludeClosed=true&amp;recherche' \\\n",
        "                    f'.adresse=&amp;recherche.raisonSociale=&amp;sirene_locale=fr '\n",
        "        wd.get(url_siret)\n",
        "        source = wd.page_source\n",
        "        soup = BeautifulSoup(source)\n",
        "        size = soup.select_one('#collapse-0 > div > div.result-right > p:nth-of-type(8) > label').next_sibling.strip()\n",
        "        try:\n",
        "            size_category = \\\n",
        "                soup.select_one(\n",
        "                    '#collapse-0 > div > div.result-right > p:nth-of-type(10) > label').next_sibling.split()[0]\n",
        "        except IndexError:\n",
        "            size_category = None\n",
        "            print(f\"Couldn't fetch size_category for company with siret {siret}.\")\n",
        "        return size, size_category\n",
        "\n",
        "    @staticmethod\n",
        "    def date_raw_to_dmy(date_raw):\n",
        "        \"\"\"Converts a timestamp to a day-month-Year date.\n",
        "        Parameters\n",
        "        ----------\n",
        "        date_raw : int\n",
        "            timestamp (e.g. 1624028410).\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            date with d-m-Y format.\n",
        "        \"\"\"\n",
        "        if date_raw < 0 :\n",
        "            date_raw += 63113904000000 # 2000 years in milliseconds ; the reason is that the date is encoded as 0021\n",
        "            # for example instead of 2021.\n",
        "        your_dt = datetime.datetime.fromtimestamp(int(date_raw) / 1000)  # using the local timezone\n",
        "        return your_dt.strftime(\"%d-%m-%Y\")\n",
        "\n",
        "    def get_agreements_infos(self, user_feedback=True, start_at=0):\n",
        "        \"\"\"Gets a complete set of information for each agreement on the chosen scope of agreements.\n",
        "        Information that are gathered for each agreement are : dateDepot, dateDiffusion, dateEffet, dateFin, dateMaj,\n",
        "        dateTexte, company name, siret, size, size category, union, sector, nature, themes, NAF code and the agreement\n",
        "        content.\n",
        "        It enriches the list_agreement attribute with additional information using the Legifrance API:\n",
        "        https://developer.aife.economie.gouv.fr/.\n",
        "        Parameters\n",
        "        ----------\n",
        "        user_feedback : :obj:`bool`, optional\n",
        "            Set to True if the user wishes to have feedback on the advancement of the scrapping. (Defaults to True)\n",
        "        start_at : :obj:`int`, optional\n",
        "            Number of the agreement you wish to start with for the scrapping. Useful when your session crashed and you\n",
        "            don't want to start again from the beginning. (Default value = 0)\n",
        "        Raises\n",
        "        ------\n",
        "        JSONDecodeError\n",
        "            An error occurs while you are trying to access the Legifrance API. The json decoding is not actually the\n",
        "            issue, it is rather that the API communication failed and that you most likely need to get a fresh token on\n",
        "            the API. This error can only be raised if the user decides to exit the program (by entering 'Enter' when\n",
        "            they are asked to).\n",
        "        Notes\n",
        "        -----\n",
        "        Should only be ran after ScrapAgreement.get_all_pages_ids.\n",
        "        \"\"\"\n",
        "        n = len(self.list_agreement)\n",
        "        if user_feedback:\n",
        "            self.count_1 = 0\n",
        "            count_2 = 0\n",
        "        for i in range(start_at, n):\n",
        "            self.position = i  # Useful for recovering the position at which you stopped, in case your session crashed\n",
        "            # or raised an error.\n",
        "            agree = self.list_agreement[i]\n",
        "            agreement_id = agree['id']\n",
        "\n",
        "            headers = {'Authorization': f'Bearer {self.token_legifrance}'}\n",
        "            data = {\"id\": agreement_id}\n",
        "            count_requests_fail = 0\n",
        "            bool_break = 1  # When set to 1, the while loop below won't break unless we get a positive result or the\n",
        "            # user decides to stop the algorithm.\n",
        "            while bool_break:\n",
        "                response = requests.post(\n",
        "                    'https://sandbox-api.piste.gouv.fr/dila/legifrance-beta/lf-engine-app/consult/acco',\n",
        "                    headers=headers,\n",
        "                    json=data)\n",
        "                try:\n",
        "                    response_json = response.json()\n",
        "                    bool_break = 0\n",
        "                except JSONDecodeError:\n",
        "                    if count_requests_fail < 11:\n",
        "                        count_requests_fail += 1\n",
        "                    else:\n",
        "                        user_request = input(\n",
        "                            '  Please refresh your Legifrance token. Enter the new token there : \\n'\n",
        "                            '(if you do not wish to refresh your token and wish to exit the scrapping, do not type '\n",
        "                            'anything there and press Enter) \\n')\n",
        "                        if user_request != '':\n",
        "                            print('Token well received.')\n",
        "                            self.token_legifrance = user_request\n",
        "                            headers = {'Authorization': f'Bearer {self.token_legifrance}'}\n",
        "                        else:\n",
        "                            print(\"Scrapping via Legifrance API failed for agreement with id:\", agreement_id)\n",
        "                            print(\"response value:\", response)\n",
        "                            raise JSONDecodeError\n",
        "            try:\n",
        "                dateDepot_raw = response_json['acco']['dateDepot']  # deposit date on legifrance\n",
        "                dateDepot = self.date_raw_to_dmy(dateDepot_raw)\n",
        "                self.list_agreement[i]['dateDepot'] = dateDepot\n",
        "\n",
        "                dateDiffusion_raw = response_json['acco']['dateDiffusion']  # date it was published on legifrance\n",
        "                dateDiffusion = self.date_raw_to_dmy(dateDiffusion_raw)\n",
        "                self.list_agreement[i]['dateDiffusion'] = dateDiffusion\n",
        "\n",
        "                dateEffet_raw = response_json['acco']['dateEffet']  # date from which the agreement takes effect\n",
        "                dateEffet = self.date_raw_to_dmy(dateEffet_raw)\n",
        "                self.list_agreement[i]['dateEffet'] = dateEffet\n",
        "\n",
        "                dateFin_raw = response_json['acco']['dateFin']  # date until which the agreement has effect\n",
        "                dateFin = self.date_raw_to_dmy(dateFin_raw)\n",
        "                self.list_agreement[i]['dateFin'] = dateFin\n",
        "\n",
        "                dateMaj_raw = response_json['acco'][\n",
        "                    'dateMaj']  # unsure what this date corresponds to. will let it as is in the dataframe\n",
        "                dateMaj = self.date_raw_to_dmy(dateMaj_raw)\n",
        "                self.list_agreement[i]['dateMaj'] = dateMaj\n",
        "\n",
        "                dateTexte_raw = response_json['acco']['dateTexte']  # date the agreement was signed\n",
        "                dateTexte = self.date_raw_to_dmy(dateTexte_raw)\n",
        "                self.list_agreement[i]['dateTexte'] = dateTexte\n",
        "\n",
        "                entreprise = response_json['acco']['raisonSociale']\n",
        "                self.list_agreement[i]['entreprise'] = entreprise\n",
        "\n",
        "                siret = response_json['acco']['siret']\n",
        "                self.list_agreement[i]['siret'] = siret\n",
        "\n",
        "                size, size_category = self.get_company_size(siret)\n",
        "                self.list_agreement[i]['size'] = size\n",
        "                self.list_agreement[i]['size_category'] = size_category\n",
        "\n",
        "                syndicats = []\n",
        "                for syndic in response_json['acco']['syndicats']:\n",
        "                    syndicats.append(syndic['libelle'])\n",
        "                self.list_agreement[i]['syndicats'] = syndicats\n",
        "\n",
        "                secteur = response_json['acco']['secteur']\n",
        "                self.list_agreement[i]['secteur'] = secteur\n",
        "\n",
        "                nature = response_json['acco']['nature']\n",
        "                self.list_agreement[i]['nature'] = nature\n",
        "\n",
        "                themes = []\n",
        "                for theme in response_json['acco']['themes']:\n",
        "                    themes.append(theme['libelle'])\n",
        "                self.list_agreement[i]['themes'] = themes\n",
        "\n",
        "                codeNAF = response_json['acco']['codeApe']\n",
        "                self.list_agreement[i]['codeNAF'] = codeNAF\n",
        "\n",
        "                contenu = response_json['acco']['attachment']['content']\n",
        "                self.list_agreement[i]['contenu'] = contenu\n",
        "\n",
        "            except KeyError:  # If an agreement cannot be fetched due to an \"Le nombre de résultat retourné par le\n",
        "                # moteur de recherche n'est pas égal à 1.\" error\n",
        "                print(f\"Agreement {agreement_id}'s information couldn't be fetched at all.\")\n",
        "\n",
        "            if user_feedback:\n",
        "                self.count_1 += 1\n",
        "                count_2 += 1\n",
        "                if count_2 == 500:\n",
        "                    print(f\"We're at {self.count_1 / n * 100}%.\")\n",
        "                    count_2 = 0\n",
        "\n",
        "    def auto_scrap(self, save_path=None, saving_format='xlsx', start_at=0, skip_to_saving=False):\n",
        "        \"\"\"Compiles all the scrapping methods in one. Does all the scrapping at once.\n",
        "        Parameters\n",
        "        ----------\n",
        "        save_path : str\n",
        "            Path you wish the .csv database be saved to. Please remove any '\\' or '/' at the end. For example, it can be\n",
        "            under the form of `C :/User/documents/folder`. (Default value = None)\n",
        "        saving_format : :obj:`str`, optional\n",
        "            Should be set either as `xlsx` or `csv`. (Default value = `xlsx`)\n",
        "        start_at : :obj:`int`, optional\n",
        "            Number of the agreement you wish to start with for the scrapping. Useful\n",
        "            when your session crashed and you don't want to start again from the beginning. If set as\n",
        "            higher than 0, will automatically skip the get_all_pages_ids method as we assume you've already finished it.\n",
        "            (Default value = 0)\n",
        "        skip_to_saving : :obj:`bool`, optional\n",
        "            If, as a user, you only failed when entering the saving path and do not wish to relaunch the whole thing. If\n",
        "            set as True, will directly skip to the saving part. (Default value = False)\n",
        "       Raises\n",
        "        ------\n",
        "        JSONDecodeError\n",
        "            An error occurs while you are trying to access the Legifrance API. The json decoding\n",
        "            is not actually the issue, it is rather that the API communication failed and that you most likely need\n",
        "            to get a fresh token on the API. This error can only be raised if the user decides to exit the program (by\n",
        "            entering 'Enter' when they are asked to).\n",
        "        FileNotFoundError\n",
        "            You entered a wrong save_path.\n",
        "        \"\"\"\n",
        "        time1 = time.time()\n",
        "        if skip_to_saving is False:\n",
        "            if start_at == 0:\n",
        "                self.get_all_pages_ids()\n",
        "                print(\"All pages IDs fetched.\")\n",
        "            self.get_agreements_infos(start_at=start_at)\n",
        "        self.df = pd.DataFrame(self.list_agreement)\n",
        "        try:\n",
        "            if saving_format == 'xlsx':\n",
        "                self.df.to_excel(\n",
        "                    save_path + f'/legifrance_database_{self.agree_subject}_{self.agree_type}.{saving_format}')\n",
        "            elif saving_format == 'csv':\n",
        "                self.df.to_csv(\n",
        "                    save_path + f'/legifrance_database_{self.agree_subject}_{self.agree_type}.{saving_format}')\n",
        "            else:\n",
        "                raise ValueError(\"Please choose 'xlsx' or 'csv' as the saving_format for your scrapped database.\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError('Please enter a valid save_path for your file. You do not need to relaunch the '\n",
        "                                    'whole process, please set skip_to_saving to True in the attributes to directly'\n",
        "                                    'skip to saving the dataframe.')\n",
        "\n",
        "        print(f\"100%.\\nScrapping done. It took {time.time() - time1} seconds.\")\n",
        "\n",
        "my_scrap = ScrapAgreement(agree_subject = agree_subject, agree_type = agree_type, token_legifrance = token_legifrance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJTM-uO958LX"
      },
      "source": [
        "And there it is : the final cell. Launch it to get the full scrapping. Should take between 1 and 2 hours, with constant user feedbacks. Stay tuned, as it is possible that you need to renew your Legifrance token during the process (the algorithm will inform you if that's the case and ask you the new token)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc76sfSj4mhg"
      },
      "source": [
        "my_scrap.auto_scrap(save_path = save_path, saving_format='xlsx', start_at=0, skip_to_saving=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk-fP-Qp6Od4"
      },
      "source": [
        "*End note:*    \n",
        "If you entered, let's say, an invalid `save_path` and that your scrapping crashes during the saving process, well... Don't worry, just launch the above cell again with the new `save_path`, and by setting the `skip_to_saving` parameter to **True**."
      ]
    }
  ]
}